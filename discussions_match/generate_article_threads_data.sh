#!/bin/bash

# define locations of input files
# make sure to have run cache_revisions.py -w sections
discussions_compact_text='../../WikiTalkParser/discussions/discussions_text.csv'
thread_titles='../../WikiTalkParser/discussions/thread_titles.csv'
thread_metrics_tree_string='../../ContropediaTalk/discussion-metrics/thread_metrics.csv'
article_ids_titles='/home/erik/contropedia/talk_files/Article_ids_titles.csv'
cache_dir='/var/www/contropedia/demo/experiment/cache'

# LOAD MYSQL CONFIG
#. ./db.inc
. ../../config.cfg

# loop over set of files listed in Article_ids_titles.csv
for page in $(cut -f2 $article_ids_titles) 
do 
    echo "doing [$page]"

    # TAKE PAGE ARG AS INPUT
    datadir="$cache_dir/$page/discussions"
    if [ ! -e "$datadir" ] || [ ! -d "$datadir" ];
    then
        mkdir $datadir
    fi

    # get list of revisions for an article and make revisions.tsv
    `sudo rm $tmpdir/revisions.tsv`
    `echo "SELECT r.id, r.user, r.timestamp, r.hash, REPLACE(r.comment, '\n', ' ') AS comment FROM revisions r, article a, article_revisions ar WHERE r.id = ar.revision_id AND ar.article_id = a.id AND a.title = '${page}' INTO OUTFILE '$(tmpdir)/revisions.tsv' FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n'" | mysql -u $MYSQLUSER -p$MYSQLPASS $MYSQLDB`
    echo "rev_id\trev_user\trev_timestamp\trev_hash\trev_comment" > "$datadir/revisions.tsv"
    `cat $tmpdir/revisions.tsv >> $datadir/revisions.tsv`

    revisions_ids=$(echo "SELECT ar.revision_id FROM article a, article_revisions ar WHERE ar.article_id = a.id AND a.title = '${page}'" | mysql -u $MYSQLUSER -p$MYSQLPASS $MYSQLDB | sed '1d')

    # @TODO store pageid in database, instead of retrieving it here
    pageid=$(curl -f -s -L "http://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&rvlimit=1&titles=$page" | sed 's/.*pageid"://g' | sed 's/,.*//g')

    # Extract list of sections in each revision of the page
    # @todo, instead of downloading via cache_revisions.py -w sections, we could also parse out the sections ourselves from cache_revisions.py -w wikitext
    #if [ ! -s "$datadir/sections.tsv" ]; then # @todo, ! -s does not allow for update
      rm -f "$datadir/sections.tmp"
      for revid in $revisions_ids; do 
        if [ -e "$datadir/../sections/$revid" ]; # look for cached version of section information, as generated by cache_revisions.py -w wikitext
        then
            cat "$datadir/../sections/$revid" | sed -e $'s/","number/\\\n/g' | grep -v ']}}' | sed 's/^.*"line":"//' | sed 's/^\(.*\)$/\1/' >> "$datadir/sections.tmp"
        else
            echo "$datadir/../sections/$revid could not be found"
        fi       
      done
      sort -u "$datadir/sections.tmp" > "$datadir/sections.tsv"
      rm "$datadir/sections.tmp"
    #fi

    # Get the association of revisions with sections from the Contropedia database
    # original
    # select=$(echo "SELECT to_revision_id as revision_id, raw_element as section_name FROM element_edit ee, article_revisions ar, article a WHERE ee.to_revision_id = ar.revision_id AND ar.article_id = a.id AND a.title = '${page}' GROUP BY revision_id, section_name")
    # better, as it actually matches section_name's instead of canonical elements?
    select=$(echo "SELECT ee.to_revision_id AS revision_id, s.section_name AS section_name FROM element_edit ee, section s, article_revisions ar, article a WHERE ee.section_id = s.id AND ee.to_revision_id = ar.revision_id AND ar.article_id = a.id AND a.title = '${page}' GROUP BY revision_id, section_name")
    echo "$select" | mysql -u $MYSQLUSER -p$MYSQLPASS $MYSQLDB > "$datadir/revisions_sections.tsv"

    # Extract discussions
    head -n 1 $discussions_compact_text | iconv -f "iso8859-1" -t "UTF-8" > "$datadir/discussions.tsv"
    grep -P "^([^\t]+\t){5}$pageid\t" $discussions_compact_text | iconv -f "iso8859-1" -t "UTF-8" >> "$datadir/discussions.tsv"

    # Add missing thread_title column
    if ! head -n 1 "$datadir/discussions.tsv" | grep -P "\tthread_title$" > /dev/null; then
      ./add_thread_column.sh "$datadir/discussions.tsv" > "$datadir/discussions.tsv.new"
      mv -f "$datadir/discussions.tsv.new" "$datadir/discussions.tsv"
    fi

    # Extract discussions metrics
    head -n 1 $thread_metrics_tree_string | iconv -f "iso8859-1" -t "UTF-8" > "$datadir/threads_metrics.tsv"
    grep -P "^$pageid\t" $thread_metrics_tree_string | iconv -f "iso8859-1" -t "UTF-8" >> "$datadir/threads_metrics.tsv"

    # Extract thread permalinks
    head -n 1 $thread_titles | iconv -f "iso8859-1" -t "UTF-8" > "$datadir/threads_links.tsv"
    grep -P "^$pageid\t" $thread_titles | iconv -f "iso8859-1" -t "UTF-8" >> "$datadir/threads_links.tsv"

    # Extract actors from Contropedia database
    echo "SELECT e.canonical FROM element e LEFT JOIN element_edit ee ON ee.element_id = e.id LEFT JOIN section s ON ee.section_id = s.id LEFT JOIN revisions r ON ee.to_revision_id = r.id LEFT JOIN article_revisions ar ON ar.revision_id = r.id LEFT JOIN article a ON ar.article_id = a.id WHERE a.title = '$page' GROUP BY canonical ORDER BY canonical" | mysql -u $MYSQLUSER -p$MYSQLPASS $MYSQLDB > "$datadir/actors.tsv"

    # Match discussions with article sections and actors, and assemble all data into $datadir/threads_matched.csv and $datadir/actors_matched.csv
    echo "matching discussions for $page"
    python match_discussions_sections.py "$page"
done